{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TRy7rR3-GAu"
   },
   "source": [
    "Whith this exercise we will practice on the most common methodologies for features processing and features engineering\n",
    "# Features Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kySlDFrp-AcJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KahnU2fu-abS"
   },
   "source": [
    "## Handling Outliers\n",
    "Common ways to identify the presence of outliers is to visualize the data or to calculate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pY4aSKYlTg4M"
   },
   "outputs": [],
   "source": [
    "# visualizing boxplot\n",
    "import seaborn as sns\n",
    "sns.boxplot(x=data['column_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYbi6MVfUUGM"
   },
   "outputs": [],
   "source": [
    "#discovering outliers with z-score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "z = np.abs(stats.zscore(data[[\"column_name\"]]))\n",
    "threshold = 3\n",
    "outliers_indexes = np.where(z > 3) #array of indexes of the samples with zscore > 3 (outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Srw60rm2U9WG"
   },
   "outputs": [],
   "source": [
    "##discovering outliers with IQR-score\n",
    "Q1 = data[\"column_name\"].quantile(0.05)\n",
    "Q3 = data[\"column_name\"].quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)\n",
    "\n",
    "logical_index_outliers = data[\"column_name\"] < (Q1 - 1.5 * IQR)) |(data[\"column_name\"] > (Q3 + 1.5 * IQR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nk6wQ6lSeQM"
   },
   "source": [
    "### Outlier detection with standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMuYu-1tSdJc"
   },
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "k = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * k\n",
    "lower_lim = data['column'].mean () - data['column'].std () * k\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)] #Drop outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LInL-fmASuDT"
   },
   "source": [
    "### Outlier detecion with percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7for8Qg-X-d"
   },
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)] #Drop outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGgezPtETNEl"
   },
   "source": [
    "### Cap instead of Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nqcQlTJTRf7"
   },
   "outputs": [],
   "source": [
    "#Capping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7_cPpmI-h7p"
   },
   "source": [
    "## Handling Missing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd42-bvRKc81"
   },
   "outputs": [],
   "source": [
    "# shows the type of data and the existence of missing values\n",
    "data.info()\n",
    "# or show the summary statistics of the complete dataset for all features in the dataset\n",
    "data.describe(include=\"all\")\n",
    "# or count the missing values per features and flattened\n",
    "sum(isnan(data).flatten())\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe_3siT4FuWg"
   },
   "source": [
    "### Eliminate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zb9k4TiF-k5S"
   },
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyCi2kGiFlqL"
   },
   "source": [
    "### Numerical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiRo-9QpFk5o"
   },
   "outputs": [],
   "source": [
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "#Filling missing values with mean, median, mode of the columns\n",
    "data = data.fillna(data.mean())\n",
    "data = data.fillna(data.median())\n",
    "data = data.fillna(data.mode())\n",
    "\n",
    "# Filling with interpolation\n",
    "data['column_name'].interpolate(method='linear', limit=2, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "273YxaVkIcq1"
   },
   "source": [
    "### Cathegorical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTkiPWBwIhaD"
   },
   "outputs": [],
   "source": [
    "#Filling missing values on categorical columns with the most frequent value\n",
    "data['column_name'].fillna(data['column_name'].value_counts().idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpTJye15I0I_"
   },
   "source": [
    "### Imputation using kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akW4iAj_NSxe"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "#...\n",
    "# initial total missing values\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = KNNImputer(n_neighbors=5, weights='distance', metric='nan_euclidean')\n",
    "# NaN-aware Euclidean Distance: does not include NaN values when calculating the distance between members \n",
    "# of the training dataset.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n",
    "\n",
    "# fit the imputer on the dataset\n",
    "imputer.fit(X[[\"col1\",\"col2\",\"col3\",\"col4\"]])\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X[[\"col1\",\"col2\",\"col3\",\"col4\"]])\n",
    "\n",
    "# fit_transform\n",
    "Xtrans = pd.DataFrame(imputer.fit_transform(X[[\"col1\",\"col2\",\"col3\",\"col4\"]]))\n",
    "\n",
    "# final total missing values\n",
    "print('Missing: %d' % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXUE6E-PI7JF"
   },
   "source": [
    "### Regression based Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjRhDK4-JOm4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "test_data = data[data['column_name'].isnull()==True]\n",
    "train_data = data[data['column_name'].isnull()==False]\n",
    "\n",
    "y = train_data['column_name'] #target is \"column_name\"\n",
    "train_data.drop(\"column_name\",axis=1,inplace=True) #features are all other features except \"column_name\"\n",
    "\n",
    "lr_model.fit(train_data,y)\n",
    "\n",
    "test_data.drop(\"column_name\",axis=1,inplace=True)\n",
    "\n",
    "# infer the missing values with the learned model\n",
    "pred = lr_model.predict(test_data)\n",
    "test_data['column_name']= pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVbrpPBk-lyS"
   },
   "source": [
    "## Processing and Scaling\n",
    "We will use the `cancer` dataset available on `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FAlk_hb-osD"
   },
   "outputs": [],
   "source": [
    "## LOAD the dataset\n",
    "from sklearn.datasets import load_breast_cancer \n",
    "from sklearn.model_selection import train_test_split \n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)\n",
    "print(X_train.shape) \n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UckzAEUo0lTD"
   },
   "source": [
    "*   The dataset contains 569 data points, each represented by 30 measurements\n",
    "*   We split the dataset into training set and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k5p6K00zoPy"
   },
   "source": [
    "### Normalization (MinMax Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCtONwgK-tn0"
   },
   "outputs": [],
   "source": [
    "# first import the class necessary for the preprocessing, and then instantiate it\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "#X_train_scaled = scaler.fit_transform(X_train)  # do this to fit and transform in the same command\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "# DO NOT TRAIN a new scaler on test data (all the above but with the test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq99856V06Rg"
   },
   "source": [
    "> *   We **fit** the scaler on the **training data**. \n",
    "> *   For the *MinMaxScaler*, the `fit` method computes the ***minimum*** and ***maximum*** value of each feature on the `training` set. \n",
    "> *   We call fit on the `training` set, and then call transform on the `training` and `test` sets\n",
    "> *   For the test set, after scaling, the minimum and maximum values are not 0 and 1. Some of the features will be even outside the 0–1 range! The explanation is that the `MinMaxScaler` (and all the other scalers) always applies exactly the same transformation to the training and the test set. This means the transform method always subtracts the training set minimum and divides by the training set range, which might be different from the minimum and range for the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alQWuVwB4Nzp"
   },
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tM2DhyZ4mTX"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "print(scaler.mean_) #The mean value for each feature in the training set\n",
    "print(scaler.scale_) #Per feature relative scaling of the data to achieve zero mean and unit variance\n",
    "\n",
    "X_scaled = scaler.transform(X_train)\n",
    "print(X_scaled.mean(axis=0)) #Zero mean\n",
    "print(X_scaled.std(axis=0)) #unit variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZzE4NhV6BvX"
   },
   "source": [
    "### Robust Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyMWZwQu6oYd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "X = [[ 1., -2.,  2.],\n",
    "    [ -2.,  1.,  3.],\n",
    "    [ 4.,  1., -2.]]\n",
    "transformer = RobustScaler().fit(X)\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUlpsAjN9Dal"
   },
   "source": [
    "# PRACTICING on Wine dataset\n",
    "The dataset is available on iCorsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SYlofLhAO-P"
   },
   "source": [
    "#### 1. Check the presence of outliers and identify the best way to handle them\n",
    "#### 2. Verify if the dataset presents missing values and handle them\n",
    "#### 3. Verify the range and location of the features values and if necessary scale them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDfPxRzOCMWX"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GH_c-UrIC6Yx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "red_df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "white_df = pd.read_csv('winequality-white.csv', sep=';')\n",
    "\n",
    "red_df['wine_type'] = 1 # let's encode red with the value 1\n",
    "white_df['wine_type'] = 0 # let's encode white with the value 0\n",
    "\n",
    "df = pd.concat([red_df, white_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['pH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##discovering outliers with IQR-score\n",
    "Q1 = df[\"pH\"].quantile(0.05)\n",
    "Q3 = df[\"pH\"].quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)\n",
    "\n",
    "logical_index_outliers = (df[\"pH\"] < (Q1 - 1.5 * IQR)) | (df[\"pH\"] > (Q3 + 1.5 * IQR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = df[\"pH\"].quantile(.95)\n",
    "lower_lim = df[\"pH\"].quantile(.05)\n",
    "\n",
    "df = df[(df[\"pH\"] < upper_lim) & (df[\"pH\"] > lower_lim)] #Drop outliers\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['pH'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_A_features_processing_vstud.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
