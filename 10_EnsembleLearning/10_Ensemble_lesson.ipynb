{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg5-CCy_g-3e"
   },
   "source": [
    "# Voting Classifiers\n",
    "In the following we create and train a voting classifier in Scikit-Learn, composed of three diverse classifiers (the training set is the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTEnCK1jiUA0"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data=make_moons(n_samples=1000)\n",
    "X=data[0]\n",
    "y=data[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l5p3VW1m_5Z"
   },
   "source": [
    "## Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSj5MB29g51U",
    "outputId": "06016e2d-f229-4779-b5ef-0ef5b23f1294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.896\n",
      "RandomForestClassifier 0.992\n",
      "SVC 1.0\n",
      "VotingClassifier 0.992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# instantiate classifiers\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "#ensemble hard voting\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='hard')\n",
    "\n",
    "#evaluate on the test set\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPeESsEUj_7H"
   },
   "source": [
    "The voting classifier might slightly outperforms all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCI9pTFXnCio"
   },
   "source": [
    "## Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bM9Imxl5kDKF",
    "outputId": "b22979d8-b957-46b1-ebf8-994df1b4c601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.896\n",
      "RandomForestClassifier 0.992\n",
      "SVC 1.0\n",
      "VotingClassifier 1.0\n"
     ]
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True) ## by nature does not provide probabilities\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='soft') ## we specify SOFT voting\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "  clf.fit(X_train, y_train)\n",
    "  y_pred = clf.predict(X_test)\n",
    "  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ar74FUSxbvI"
   },
   "source": [
    "#Bagging and Pasting\n",
    "The following code trains an ensemble of 500 Decision Tree classifiers, each trained on 100 training instances randomly sampled from the training set with replacement (this is an example of **bagging**, but if you want to use **pasting** instead, just set `bootstrap=False`). The `n_jobs` parameter tells Scikit-Learn the number of CPU cores to use for training and predictions (–1 tells Scikit-Learn to use all available cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGQIUEvInHF6"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVro_U6VyqIM"
   },
   "source": [
    "The `BaggingClassifier` automatically performs **soft voting** instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with Decision Trees classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPfhEQny1vfZ"
   },
   "source": [
    "## Out-of-bag Evaluation\n",
    "In Scikit-Learn, you can set `oob_score=True` when creating a BaggingClassifier to request an automatic oob evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ln8vxiwu1uh3",
    "outputId": "e39da32b-4ad5-4b86-e178-b1fad14a6db5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9973333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRkGPpDd2Y0G"
   },
   "source": [
    "According to this oob evaluation, this `BaggingClassifier` is likely to achieve about 99.7% accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRITrxfhyE3T",
    "outputId": "ad255dac-28af-4433-8a8f-55325291bc5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.988"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEcXXW402jis"
   },
   "source": [
    "Calculating the accuracy directly on the test set, the score is quite close 98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZvF2nRg2UNh",
    "outputId": "a6503d9d-f65a-4651-bdf4-d5a17ab8e534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06122449, 0.93877551],\n",
       "       ...,\n",
       "       [0.        , 1.        ],\n",
       "       [0.01156069, 0.98843931],\n",
       "       [0.        , 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VqDqDAW27Qi"
   },
   "source": [
    "The oob decision function for each training instance is also available through the `oob_decision_function_` variable. In this case (since the base estimator has a `predict_proba()` method) the decision function returns the class probabilities for each training instance. For example, the oob evaluation estimates that the second training instance has a 83.88% probability of belonging to the positive class (and 16.11% of belonging to the negative class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iXoc02p8MMW"
   },
   "source": [
    "# Random Forest\n",
    "Instead of building a `BaggingClassifier` and passing it a `DecisionTreeClassifier`, we can instead use the `RandomForestClassifier` class, which is more convenient and optimized for Decision Trees (similarly, there is a `RandomForestRegressor` class for regression tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D2mPm9t8YAh"
   },
   "source": [
    "The following code trains a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ljujU2823gw"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_CKsuHx8soG"
   },
   "source": [
    "A `RandomForestClassifier` has almost all the hyperparameters of a `DecisionTreeClassifier` (to control how trees are grown), plus all the hyperparameters of a `BaggingClassifier` to control the ensemble itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3zfLhrS9SN2"
   },
   "outputs": [],
   "source": [
    "# equivalent Bagging classifier\n",
    "bag_clf = BaggingClassifier( DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16), n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CH7ouFie_E1A"
   },
   "source": [
    "## ExtraTreeClassifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amE2H0y18Th3"
   },
   "outputs": [],
   "source": [
    "#ExtraTreesClassifier (trained as RandomForestClassifier)\n",
    "#ExtraTreesRegressor (trained as RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDWeLYolA3Gx"
   },
   "source": [
    "## Features Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HBPh3Zd_VdJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "  print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8xQYc7TqWq2"
   },
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu09RsdmqaUh"
   },
   "source": [
    "## AdaBoost\n",
    "The following code trains an `AdaBoost` classifier based on 200 *Decision Stumps* using Scikit-Learn’s `AdaBoostClassifier` class (there exist also an `AdaBoostRegressor` class). \n",
    "\n",
    "A **Decision Stump** is a Decision Tree with `max_depth=1` —in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the `AdaBoostClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "063mbf0WBDl0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), \n",
    "    n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", \n",
    "    learning_rate=0.5)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwXi6Jk7rEa5"
   },
   "source": [
    "Scikit-Learn uses a multiclass version of AdaBoost called `SAMME` (which stands for *Stagewise Additive Modeling using a Multiclass Exponential loss function*). When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the predictors can estimate class probabilities (i.e., if they have a `predict_proba()` method), Scikit-Learn can use a variant of SAMME called `SAMME.R` (the R stands for *“Real”*), which relies on class probabilities rather than predictions and generally performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TniYYBYpwagX"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OZBvl0PwdQO"
   },
   "source": [
    "### Gradient Boosted Regression Trees (GBRT) - manual\n",
    "(or `Gadient Tree Boosting`)\n",
    "\n",
    "First, let’s fit a DecisionTreeRegressor to the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUVfDJnwq_vy"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "# noisy quadratic training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wr1jEYpfw-r8"
   },
   "source": [
    "Train a second `DecisionTreeRegressor` on the residual errors made by the first\n",
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lbfSqLJwxf5"
   },
   "outputs": [],
   "source": [
    "#residual calculation\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "\n",
    "# train DT on residuals\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDL0KElYxTqi"
   },
   "source": [
    "Then train a third regressor on the residual errors made by the second predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN32SuQ9xW7t"
   },
   "outputs": [],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDQH0EZFxnR2"
   },
   "source": [
    "Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CVbDamjxoNK"
   },
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqpSGkiBzu_S"
   },
   "source": [
    "### Gradient Boosted Regression Trees (GBRT) - sklearn\n",
    "Here we create the same ensamble as the previous one, but we use sklearn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFstlIQrzuBd"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBUCAR0AuDD9"
   },
   "source": [
    "#### Optimal number of trees\n",
    "In order to find the optimal number of trees, we can use **early stopping**. \n",
    "\n",
    "A simple way to implement this is to use the `staged_predict()` method: it returns an iterator over the predictions made by the ensemble at each stage of training (with one tree, two trees, etc.). \n",
    "\n",
    "The following code trains a GBRT ensemble with 120 trees, then measures the validation error at each stage of training to find the optimal number of trees, and finally trains another GBRT ensemble using the optimal number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5KRYTsiuFAQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acxPw5z-w7Vu"
   },
   "source": [
    "It is also possible to implement early stopping by actually stopping training early (instead of training a large number of trees first and then looking back to find the optimal number). \n",
    "\n",
    "We can do so by setting `warm_start=True`, which makes Scikit-Learn keep existing trees when the `fit()` method is called, allowing **incremental training**. \n",
    "\n",
    "The following code stops training when the validation error does not improve for five iterations in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o5QEoLZ1wI9n"
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "min_val_error = float(\"inf\") \n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1, 120):\n",
    "  gbrt.n_estimators = n_estimators \n",
    "  gbrt.fit(X_train, y_train)\n",
    "  y_pred = gbrt.predict(X_val)\n",
    "  val_error = mean_squared_error(y_val, y_pred) \n",
    "  if val_error < min_val_error:\n",
    "    min_val_error = val_error\n",
    "    error_going_up = 0 \n",
    "  else:\n",
    "    error_going_up += 1\n",
    "    if error_going_up == 5:\n",
    "      break # early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is--X8SCycPh"
   },
   "source": [
    "### XGBoost\n",
    "**Extreme Gradient Boosting** aims at being extremely fast, scalable and portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDmW55l3yf04"
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tDqAX8cy1Co"
   },
   "outputs": [],
   "source": [
    "# automatic early stopping\n",
    "xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5inmu0pYzBaj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "10_Ensemble_lesson.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
